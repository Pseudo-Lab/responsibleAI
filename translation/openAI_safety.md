## OpenAI 헌장

우리 헌장은 OpenAI의 사명을 실행하는 데 사용하는 원칙을 설명합니다.

이 문서에는 OpenAI 내부 및 외부의 많은 사람들의 피드백을 포함하여 지난 2년 동안 개선된 전략이 반영되어 있습니다. AGI의 일정은 여전히 불확실하지만, 우리 헌장은 개발 전반에 걸쳐 인류의 최선의 이익을 위해 행동하도록 안내할 것입니다.

OpenAI의 임무는 인공 일반 지능(AGI)(가장 경제적으로 가치 있는 작업에서 인간을 능가하는 고도로 자율적인 시스템을 의미함)이 모든 인류에게 이익이 되도록 보장하는 것입니다. 우리는 안전하고 유익한 AGI를 직접적으로 구축하려고 시도할 것이지만, 우리의 작업이 다른 사람들이 이러한 결과를 달성하도록 돕는다면 우리의 임무가 완수된 것으로 간주할 것입니다. 이를 위해 우리는 다음 원칙을 준수합니다.

### 광범위하게 분산된 혜택

우리는 AGI 배포를 통해 얻은 모든 영향력을 사용하여 그것이 모두의 이익을 위해 사용되도록 하고 인류에게 해를 끼치거나 부당하게 권력을 집중시키는 AI 또는 AGI의 사용을 방지할 것을 약속합니다.
우리의 주요 신탁 의무는 인류에 대한 것입니다. 우리는 사명을 완수하기 위해 상당한 자원을 집결해야 할 것으로 예상하지만, 광범위한 이익을 저해할 수 있는 직원과 이해관계자 간의 이해 상충을 최소화하기 위해 항상 부지런히 행동할 것입니다.

### 장기적인 안전

우리는 AGI를 안전하게 만드는 데 필요한 연구를 수행하고 AI 커뮤니티 전반에 걸쳐 이러한 연구가 광범위하게 채택되도록 하기 위해 최선을 다하고 있습니다.

우리는 적절한 안전 예방 조치를 취할 시간이 없이 후기 단계의 AGI 개발이 경쟁 레이스가 되는 것을 우려하고 있습니다. 따라서 가치와 안전을 고려한 프로젝트가 우리보다 먼저 AGI 구축에 가까워지면 우리는 이 프로젝트와 경쟁을 중단하고 지원을 시작할 것을 약속합니다. 구체적인 내용은 사례별로 합의해 나가겠지만, 일반적인 촉발 조건은 "향후 2년 내 성공 가능성이 훨씬 더 높다"는 것일 수 있습니다.

### 기술 리더십

AGI가 사회에 미치는 영향을 효과적으로 해결하려면 OpenAI가 AI 기능의 최첨단에 있어야 합니다. 정책과 안전 옹호만으로는 충분하지 않습니다.
우리는 AI가 AGI 이전에 광범위한 사회적 영향을 미칠 것이라고 믿으며, 우리의 사명과 전문성에 직접적으로 부합하는 분야를 선도하기 위해 노력할 것입니다.

### 협력 오리엔테이션

우리는 다른 연구 및 정책 기관과 적극적으로 협력할 것입니다. 우리는 AGI의 글로벌 과제를 해결하기 위해 함께 일하는 글로벌 커뮤니티를 만들기 위해 노력하고 있습니다.

우리는 사회가 AGI로 나아가는 데 도움이 되는 공공재를 제공하기 위해 최선을 다하고 있습니다. 현재 여기에는 대부분의 AI 연구 출판이 포함되지만, 안전 및 보안에 대한 우려로 인해 앞으로는 전통적인 출판이 줄어들면서 안전, 정책 및 표준 연구 공유의 중요성이 높아질 것으로 예상됩니다.

## Alignment 연구에 대한 접근 방식

우리는 인간의 피드백을 통해 학습하고 인간이 AI를 평가할 수 있도록 지원하는 AI 시스템의 능력을 개선하고 있습니다. 우리의 목표는 다른 모든 alignment 문제를 해결하는 데 도움이 될 수 있도록 충분히 aligned한 AI 시스템을 구축하는 것입니다.

우리의 alignment 연구는 인공 일반 지능(AGI)을 인간의 가치에 맞추고 인간의 의도를 따르도록 하는 것을 목표로 합니다. 우리는 반복적이고 경험적인 접근 방식을 취합니다. 고성능 AI 시스템을 align하려고 시도함으로써 우리는 무엇이 작동하고 무엇이 작동하지 않는지 학습할 수 있으며 이를 통해 AI 시스템을 보다 안전하고 보다 효과적으로 조정하는 능력을 개선할 수 있습니다. 과학적인 실험을 통해 alignment 기술이 어떻게 확장되고 어디에서 중단되는지 연구합니다.

우리는 가장 유능한 AI 시스템의 alignment 문제와 AGI로 가는 길에서 직면할 것으로 예상되는 alignment 문제를 모두 해결합니다. 우리의 주요 목표는 현재의 alignment 아이디어를 최대한 추진하고, 그것이 어떻게 성공할 수 있는지, 왜 실패할 것인지를 정확하게 이해하고 문서화하는 것입니다. 우리는 근본적으로 새로운 alignment 아이디어가 없더라도 alignment 연구 자체를 실질적으로 발전시키기 위한 충분히 aligned한 AI 시스템을 구축할 수 있다고 믿습니다.

[Unaligned AGI는](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) 인류에게 상당한 위험을 초래할 수 있으며 AGI alignment 문제를 해결하는 것은 너무 어려워서 모든 인류가 함께 협력해야 할 수 있습니다. 따라서 우리는 안전할 때 alignment 연구를 공개적으로 공유하기 위해 최선을 다하고 있습니다. 우리는 alignment 기술이 실제로 얼마나 잘 작동하는지 투명하게 밝히고 모든 AGI 개발자가 세계 최고의 alignment 기술을 사용하기를 원합니다.

높은 수준에서 alignment 연구에 대한 우리의 접근 방식은 인간의 의도에 맞춰 조정되는 매우 스마트한 AI 시스템을 위한 확장 가능한 학습 신호를 엔지니어링하는 데 중점을 둡니다. 여기에는 세 가지 주요 기둥이 있습니다.

1. 인간의 피드백을 활용한 AI 시스템 학습
2. 인간의 평가를 지원하는 AI 시스템 학습
3. alignment 연구를 수행하기 위한 AI 시스템 학습

AI 시스템을 인간의 가치에 맞추는 것은 이러한 시스템을 누구에게 맞춰야 하는지 결정하는 것과 같은 다양한 다른 중요한 사회기술적 과제도 제기합니다. 이러한 문제를 해결하는 것은 우리의 사명을 달성하는 데 중요하지만 이 게시물에서는 이에 대해 논의하지 않습니다.

### 인간의 피드백을 활용한 AI 시스템 학습

인간 피드백의 RL은 오늘날 배포된 언어 모델을 aligning하는 주요 기술입니다. 우리는 GPT-3과 같은 사전 학습된 언어 모델에서 파생된 InstructGPT라는 모델 클래스를 학습합니다. 이러한 모델은 인간의 의도(명령에 의해 제공되는 명시적 의도뿐만 아니라 진실성, 공정성, 안전과 같은 암시적 의도)를 따르도록 학습됩니다.

우리의 결과는 현재 alignment 중심의 미세 조정에 쉬운 결과가 많이 있음을 보여줍니다. InstructGPT는 100배 더 큰 사전 학습된 모델보다 인간에게 선호되는 반면 미세 조정 비용은 GPT-3 사전 학습 컴퓨팅의 2% 미만이며 인간의 피드백은 약 20,000시간에 달합니다. 우리의 작업이 업계의 다른 사람들에게 영감을 주어 대규모 언어 모델 alignment에 대한 투자를 늘리고 배포된 모델의 안전성에 대한 사용자의 기대치를 높이는 데 도움이 되기를 바랍니다.

당사의 자연어 API는 alignment 연구에 매우 유용한 환경입니다. 고객이 기꺼이 지불할 매우 다양한 작업 세트를 기반으로 정렬 기술이 실제로 현실 세계에서 얼마나 잘 작동하는지에 대한 풍부한 피드백 루프를 제공합니다. 평균적으로 고객은 이미 사전 학습된 모델보다 InstructGPT를 선호합니다.

그러나 오늘날의 InstructGPT 버전은 완전히 aligned한 것과는 거리가 멀습니다. 때로는 간단한 지침을 따르지 않고, 항상 진실하지는 않으며, 유해한 작업을 안정적으로 거부하지 않으며, 때로는 편향되거나 유해한 반응을 제공합니다. 일부 고객은 InstructGPT의 응답이 사전 학습된 모델보다 훨씬 덜 창의적이라고 생각하는데, 이는 우리가 공개적으로 사용 가능한 벤치마크에서 InstructGPT를 실행했을 때 깨닫지 못한 것입니다. 우리는 또한 인간 피드백으로부터 RL에 대한 보다 자세한 과학적 이해를 개발하고 인간 피드백의 품질을 향상시키는 방법을 개발하기 위해 노력하고 있습니다.

API를 align하는 것은 AGI를 align하는 것보다 훨씬 쉽습니다. API의 대부분 작업은 사람이 감독하기가 그리 어렵지 않고 배포된 언어 모델이 사람보다 똑똑하지 않기 때문입니다. 우리는 인간 피드백의 RL이 AGI를 정렬하는 데 충분할 것이라고 기대하지 않지만, 이는 우리가 가장 기대하는 확장 가능한 alignment 제안의 핵심 구성 요소이므로 이 방법론을 완성하는 것이 중요합니다.

### 인간의 평가를 지원하는 학습 모델

인간 피드백을 통한 RL에는 근본적인 한계가 있습니다. 즉, 인간이 AI 시스템이 수행하는 작업을 정확하게 평가할 수 있다고 가정한다는 것입니다. 오늘날 인간은 이 일에 꽤 능숙하지만 모델의 능력이 향상되면 인간이 평가하기 훨씬 어려운 작업(예: 대규모 코드베이스 또는 과학 논문에서 모든 결함 찾기)을 수행할 수 있습니다. 우리 모델은 인간 평가자에게 진실을 말하는 대신 듣고 싶은 것을 말하는 법을 배울 수도 있습니다. alignment를 확장하기 위해 우리는 RRM(재귀 보상 모델링), 토론, 반복 증폭과 같은 기술을 사용하려고 합니다.

현재 우리의 주요 방향은 RRM을 기반으로 합니다. 즉, 인간이 직접 평가하기에는 너무 어려운 작업에 대해 모델을 평가할 때 인간을 도울 수 있는 모델을 학습합니다. 예를 들어:

- 우리는 책을 요약하는 모델을 학습했습니다. 인간이 책에 익숙하지 않은 경우 책 요약을 평가하는 데 오랜 시간이 걸리지만, 우리 모델은 chapter 요약을 작성하여 인간 평가를 도울 수 있습니다.
- 우리는 웹을 검색하고 인용문과 링크를 제공하여 인간이 사실의 정확성을 평가할 수 있도록 지원하는 모델을 교육했습니다. 간단한 질문의 경우 이 모델의 출력은 이미 인간이 작성한 응답보다 선호됩니다.
- 우리는 자체 출력에 대해 중요한 댓글을 작성하도록 모델을 학습시켰습니다. 쿼리 기반 요약 작업에서 중요한 댓글에 대한 지원은 사람이 모델 출력에서 발견한 결함을 평균 50% 증가시킵니다. 이는 그럴듯해 보이지만 부정확한 요약을 작성하도록 인간에게 요청하는 경우에도 적용됩니다.
- 우리는 도움을 받지 않은 사람이 안정적으로 평가하기 매우 어려운 일련의 코딩 작업을 만들고 있습니다. 우리는 이 데이터 세트를 곧 공개할 수 있기를 바랍니다.

AI 시스템이 매우 창의적인 솔루션(예: AlphaGo의 37수)을 제안하는 경우에도 alignment 기술이 작동해야 합니다. 따라서 우리는 인간이 올바른 솔루션과 오해의 소지가 있거나 기만적인 솔루션을 구별할 수 있도록 지원하는 학습 모델에 특히 관심이 있습니다. 우리는 AI 지원 평가 작업을 실제로 수행하는 방법에 대해 가능한 한 많이 배우는 가장 좋은 방법은 AI 보조자를 구축하는 것이라고 믿습니다.

### AI system으로 Alignment 연구시키기

현재 alignment 문제에 대해 무한정 확장 가능한 솔루션은 없습니다. AI가 계속 발전함에 따라 현재 시스템에서 아직 관찰되지 않은 여러 가지 새로운 alignment 문제가 발생할 것으로 예상됩니다. 이러한 문제 중 일부는 현재 예상되며 일부는 완전히 새로운 문제입니다.

우리는 무한정 확장 가능한 솔루션을 찾는 것이 매우 어려울 것이라고 믿습니다. 대신, 우리는 인간보다 더 빠르고 더 나은 alignment 연구를 진행할 수 있는 시스템을 구축하고 정렬하는 보다 실용적인 접근 방식을 목표로 합니다.

우리가 이 작업을 진행함에 따라 AI 시스템은 점점 더 많은 alignment 작업을 대신할 수 있으며 궁극적으로 지금보다 더 나은 alignment 기술을 구상, 구현, 연구 및 개발할 수 있습니다. 그들은 자신들의 후계자들이 인간들과 더욱 일치하도록 하기 위해 인간들과 함께 일할 것입니다.

우리는 특히 평가 지원이 제공될 때 alignment 연구를 평가하는 것이 이를 생성하는 것보다 훨씬 쉽다고 믿습니다. 따라서 인간 연구자들은 스스로 연구를 생성하는 대신 AI 시스템이 수행한 alignment 연구를 검토하는 데 점점 더 많은 노력을 집중할 것입니다. 우리의 목표는 모델을 align하여 alignment 연구에 필요한 거의 모든 인지 노동을 오프로드할 수 있도록 학습시키는 것입니다.

중요한 것은 alignment 연구에서 인간뿐만 아니라 관련 영역에서 인간 수준의 능력을 갖춘 "더 좁은" AI 시스템만 필요하다는 것입니다. 우리는 이러한 AI 시스템이 범용 시스템보다 align하기 쉽고 인간보다 훨씬 똑똑한 시스템이 될 것으로 기대합니다.

언어 모델은 인터넷을 통해 인간의 가치에 대한 많은 지식과 정보가 "사전 로드"되어 있기 때문에 alignment 연구 자동화에 특히 적합합니다. 기본적으로 그들은 독립적인 대리인이 아니므로 세상에서 자신의 목표를 추구하지 않습니다. alignment 연구를 수행하기 위해 인터넷에 대한 무제한 액세스가 필요하지 않습니다. 그러나 많은 alignment 연구 작업은 자연어 또는 코딩 작업으로 표현될 수 있습니다.

WebGPT, InstructGPT 및 Codex의 향후 버전은 alignment 연구 보조자로서의 기반을 제공할 수 있지만 아직은 그 능력이 충분하지 않습니다. 우리 모델이 언제 alignment 연구에 의미 있게 기여할 수 있을 만큼 충분히 능력을 갖추게 될지는 알 수 없지만, 미리 시작하는 것이 중요하다고 생각합니다. 유용할 수 있는 모델을 학습시킨 후에는 외부 alignment 연구 커뮤니티에서 해당 모델에 액세스할 수 있도록 할 계획입니다.

### 제한 사항

우리는 AGI 조정을 위한 이러한 접근 방식에 대해 매우 기쁘게 생각하지만, AI 기술이 어떻게 발전하는지에 대해 더 많이 알게 되면 이를 적용하고 개선해야 할 것으로 기대합니다. 우리의 접근 방식에는 다음과 같은 여러 가지 중요한 제한 사항도 있습니다.

- 여기에 제시된 경로는 OpenAI가 현재 과소 투자하고 있는 두 가지 영역인 견고성과 해석 가능성 연구의 중요성을 과소평가합니다. 이것이 귀하의 프로필에 적합하다면 연구 과학자 자리에 지원하십시오!
- 평가를 위해 AI 지원을 사용하면 AI 보조자에 존재하는 미묘한 불일치, 편견 또는 취약성까지 확대되거나 증폭될 가능성이 있습니다.
- AGI alignment에는 오늘날의 AI 시스템 alignment과 매우 다른 문제를 해결하는 작업이 포함될 수 있습니다. 우리는 전환이 다소 연속적일 것으로 예상하지만, 큰 불연속성이나 패러다임 변화가 있는 경우 InstructGPT와 같은 모델 alignment에서 얻은 대부분의 교훈은 직접적으로 유용하지 않을 수 있습니다.
- alignment 문제의 가장 어려운 부분은 AI 시스템을 위한 확장 가능하고 aligned한 학습 신호를 엔지니어링하는 것과 관련이 없을 수도 있습니다. 이것이 사실이라 하더라도 그러한 학습 신호는 필요할 것이다.
- AGI를 align하는 것보다 alignment 연구를 의미 있게 가속화할 수 있는 모델을 align하는 것이 근본적으로 쉽지 않을 수 있습니다. 즉, alignment 연구에 도움이 될 수 있는 능력이 가장 낮은 모델이라도 적절하게 aligned 되지 않으면 이미 너무 위험할 수 있습니다. 이것이 사실이라면 우리는 alignment 문제를 해결하기 위해 우리 자신의 시스템으로부터 많은 도움을 얻지 못할 것입니다.

## Safety Standards

생성적 AI 도구에 대한 액세스가 인터넷 액세스만큼 필수적이 되는 사회에서 우리는 심각한 위험을 완화해야 하는 의무와 다양한 가치의 번영을 허용하는 의무 사이의 균형을 유지해야 합니다. 데이브 윌너

OpenAI의 신뢰와 안전

### 배포 안전

우리는 도구 사용을 모니터링하고 모델 위험 및 기능에 대해 학습한 내용을 기반으로 안전 완화를 업데이트하여 상용 AI 배포에 대한 리더십을 반영합니다.

## 원칙

### 피해 최소화

우리는 가능한 경우 AI 도구에 안전성을 구축하고 AI 도구의 오용이나 남용으로 인한 피해를 적극적으로 줄이기 위해 열심히 노력할 것입니다.

### 신뢰 쌓기

우리는 사용자 및 개발자 커뮤니티와 함께 우리 기술의 안전하고 유익한 응용 프로그램을 지원하는 책임을 공유할 것입니다.

### 학습 및 반복

우리는 모델이 어떻게 작동하고 사용되는지 관찰하고 분석하며 시간이 지남에 따라 시스템을 개선하기 위해 안전에 대한 접근 방식에 대한 의견을 구할 것입니다.

### 신뢰와 안전의 선두주자가 되기

우리는 생성 AI가 제기하는 고유한 신뢰 및 안전 문제에 대한 연구를 지원하여 생태계를 넘어 안전을 향상시키는 데 도움을 줄 것입니다.