## General recommended practices for AI

AI 시스템을 설계할 때는 소프트웨어 시스템에 대한 일반적인 모범 사례를 항상 따라야 하지만, 기계 학습 고유의 고려 사항도 많이 있습니다.

### Recommended practices

- 사람 중심의 디자인 방식 사용
    
    사용자가 당신의 시스템을 경험하는 방식은 그것의 예측, 추천, 결정의 실제 영향을 평가하는 데 필수적입니다.
    
    - 적절한 공개가 내장된 디자인 기능 : 명확성과 제어는 좋은 사용자 경험에 중요합니다.
    - 증강과 지원을 검토하세요 : 단일 답변을 제공하는 것은 그것이 사용자와 사용 사례의 다양성을 만족시킬 수 있는 확률을 가질 때 적절합니다. 사용자에게 몇 개의 옵션을 추천하는 것이 당신의 시스템에 적절할 지도 모릅니다. 기술적으로, 여러 답변 보다 한 개의 답변에서 좋은 precision을 달성하기는 더욱 어렵습니다.
    - 디자인 프로세스 초기에 잠재적인 부정적 피드백을 모델링한 후 전체 배포 전에 소수 트래픽에 대해 특정 실시간 테스트 및 반복을 수행합니다.
    - 다양한 사용자 및 사용 사례 시나리오를 수행하여, 프로젝트 개발 전과 전반에 걸쳐 피드백을 통합하세요. 이를 통해 프로젝트에 대한 풍부한 사용자 관점이 구축되고 기술의 혜택을 받는 사람들의 수가 늘어날 것입니다.
- 학습과 모니터링 평가에 다중 측정 항목을 식별(사용)
    
    단일 측정 항목이 아닌 다중 측정 항목을 사용하면 다양한 종류의 오류와 경험 간의 교환 관계(tradeoff)를 이해하기에 도움이 됩니다.
    
    - 사용자 설문 조사의 피드백, 단기 및 장기 제품 상태와 시스템 성능 전반을 추적하는 숫자(예: 각각 고객 평생 가치(CLV), 클릭률(CTR)), 다양한 하위 그룹에 걸친 긍정 오류 및 부정 오류 비율을 포함한 측정 항목을 고려하세요.
    - 측정 항목이 시스템의 context와 목표에 적합한지 확인하세요. 예를 들어, 화재 경보 시스템은 가금 허위 경보가 발생하더라도 재현율이 높아야 합니다.
- 가능하다면 raw data를 직접 검사
    
    ML 모델은 학습된 데이터를 반영하므로 raw data를 주의깊게 분석하여 이해했는지 확인하세요. 이것이 불가능한 경우 예를 들어, 민감한 raw data의 경우, 개인정보를 존중하면서 입력 데이터를 최대한 이해하도록 하세요. 예를 들자면, 집계된 비식별화 요약을 계산하는 등으로요.
    
    - 데이터에 실수(예: 누락된 값, 잘못된 라벨)가 포함되어 있나요?
    - 데이터가 사용자(예: 모든 연령대에 사용되지만, 노인 시민의 학습 데이터만 있음)와 실제 환경(예: 연중 사용되지만, 여름의 학습 데이터만 있음)을 나타내는 방식으로 샘플링되어 있습니까? 데이터가 정확합니까?
    - 학습-서빙 왜곡(skew) -학습과 서빙 시의 성능 차이- 는 지속적인 문제입니다. 학습 중에 잠재적인 왜곡을 식별하고 학습 데이터나 목적 함수를 조정하는 방식으로 문제를 해결하도록 하세요. 평가 시에, 배포 설정을 가능한한 대표하는 평가 데이터를 얻기 위해 지속적으로 노력하십시오.
    - 모델에 중복되거나 불필요한 Feature가 있습니까? 성능 목표를 충족하는 가장 간단한 모델을 사용하세요.
    - 지도학습 시스템의 경우, 보유하고 있는 데이터 라벨과 당신이 예측하고자 하는 아이템 간의 관계를 고려하십시오. 라벨 Y를 예측하기 위해 라벨 X를 대용물(Proxy)로 사용할 때, X와 Y 사이의 차이가(gap) 문제가 되는 경우는 무엇입니까?
    - 데이터 편향은 또다른 중요한 고려 사항입니다. AI와 공정성에 대해 더 알아보세요.
- 데이터셋과 모델의 한계를 이해
    - 상관 관계를 탐지하도록 학습된 모델을 인과 관계 추론에 사용하거나 그럴 수 있다고 암시해서는 안됩니다. 예를 들어, 모델은 농구화를 구입하는 사람들의 평균 키가 크다는 것을 학습할 수 있지만 이것이 농구화를 구입하는 사용자의 키가 결과적으로 커진다는 것을 의미하지는 않습니다.
    - 오늘날 기계 학습 모델은 대부분 학습 데이터의 패턴을 반영합니다. 따라서 학습의 범위와 적용 범위를 소통하여 모델의 성능과 한계를 분명히 하는 것이 중요합니다. 예를 들어, 재고 사진으로 학습된 신발 감지기는 재고 사진에서 가장 잘 작동할 수 있지만 사용자가 생성한 휴대폰 사진으로 테스트했을 때 제한된 성능을 가집니다.
    - 가능한 경우 사용자에게 제한 사항을 전달합니다. 예를 들어, ML을 사용하여 특정 조류 종을 인식하는 앱은 모델이 세계 특정 지역의 작은 이미지 세트에 학습되었음을 전달할 수 있습니다. 사용자를 더 잘 교육함으로써, 기능이나 어플리케이션에 대해 사용자로부터 제공되는 피드백을 개선할 수도 있습니다.
- 테스트, 테스트, 테스트
    
    소프트웨어 엔지니어링 모범 사례 및 품질 엔지니어링을 통해 AI 시스템이 의도한 대로 작동하고 신뢰할 수 있는지 확인하세요.
    
    - 엄격한 단위 테스트를 수행하여 시스템의 각 구성 요소를 개별적으로 테스트합니다.
    - 통합 테스트를 수행하여 개별 ML 구성 요소가 전체 시스템의 다른 부분과 어떻게 상호 작용하는지 이해합니다.
    - AI 시스템에 대한 입력 통계를 테스트하여 입력 드리프트(Input drift)를 사전에 감지하여 예상치 못한 방식으로 변경되지 않는지 확인합니다.
    - 완벽한 표준 데이터 세트를 사용하여 시스템을 테스트하고 예상대로 계속 작동하는지 확인하세요. 사용자 및 사용 사례 변화에 맞춰 이 테스트 세트를 정기적으로 업데이트하고 테스트 세트에 대한 학습 가능성을 이세요.
    - 개발 주기에 다양한 사용자 요구 사항을 통합하기 위해 반복적인 사용자 테스트를 수행합니다.
    - 포카요케(poka-yoke)의 품질 엔지니어링 원칙을 적용합니다. 시스템에 품질 검사를 구축하여 의도하지 않은 오류가 발생하지 않도록 하거나 즉각적인 대응을 유발하세요(예: 중요한 Feature가 예기치 않게 누락된 경우 AI 시스템이 예측을 출력하지 않음)
- 배포 이후 시스템 모니터와 업데이트 지속
    
    지속적인 모니터링을 통해 모델이 실제 세계의 성능을 가지도록 보장하고 사용자 피드백(예: 행복 추적 설문조사, HEART 프레임워크)을 고려하게 할 수 있습니다.
    
    - 문제가 발생할 것입니다 : 세계의 모든 모델은 정의상 거의 불완전합니다. 문제를 해결할 수 있도록 제품 로드맵에 시간을 투자하세요.
    - 문제에 대한 단기 및 장기 해결책을 모두 고려하십시오. 간단한 수정(예: 차단 목록 작성)은 문제를 신속하게 해결하는 데 도움이 될 수 있지만 장기적으로는 최적의 솔루션이 아닐 수도 있습니다. 단기적이고 간단한 수정과 장기적으로 학습된 솔루션의 균형을 유지하세요.
    - 배포된 모델을 업데이트하기 전에 후보 모델과 배포된 모델이 어떻게 다른지, 업데이트가 전체 시스템 품질과 사용자 경험에 어떤 영향을 미치는지 분석하세요.

## Fairness

AI 시스템은 전세계 사람들에게 새로운 경험과 능력을 제공하고 있습니다. AI 시스템은 앱, 짧은 비디오, TV 프로그램 추천 외에도 질병의 존재 및 심각도 예측, 사람과 직업 및 파트너 연결, 사람이 길을 건너고 있는지 식별하는 등 더 중요한 작업에 사용될수 있습니다. 

이러한 컴퓨터화된 보조 또는 의사 결정 시스템은 임시 규칙이나 인간의 판단에 기반한 역사적 의사 결정 프로세스보다 더 넓은 범위에서 더 공정하고 포괄적일 가능성이 있습니다. 

위험은 그러한 시스템의 불공정한 편향 역시 광범위한 영향을 미칠 수도 있다는 것입니다. 따라서 AI의 영향이 부문과 사회 전반에 걸쳐 증가함에 따라 모두를 위해 공정하고 포용적인 시스템을 향해 노력하는 것이 중요합니다.

이것은 어려운 작업입니다. 첫째, ML 모델은 현실 세계에서 수집된 기존 데이터로부터 학습하므로 모델은 인종, 성별, 종교 또는 기타 특성을 기반으로 데이터에 문제가 있는 기존 편견을 학습하거나 심지어 증폭시킬 수도 있습니다.

둘째, 가장 엄격하고 다기능적인 교육과 테스트를 수행하더라도 모든 상황이나 문화에 걸쳐 공정한 시스템을 구축하는 것은 어려운 일입니다. 예를 들어, 미국 성인을 대상으로 학습된 음성 인식 시스템은 해당 특정 상황에서 공정하고 포괄적일 수 있습니다. 그러나 십대들이 사용할 경우 시스템은 진화하는 속어나 문구를 인식하지 못할 수 있습니다. 시스템이 영국에 배포된 경우 특정 지역의 영국 억양이 다른 지역보다 더 어려울 수 있습니다. 그리고 시스템이 미국 성인에게 적용되더라도, 말을 잘 처리하지 못하는 인구의 예상치 못한 부분(예: 말더듬이가 있는 사람)을 발견할 수도 있습니다. 출시 후 시스템을 사용하면 예측하기 어려웠던 의도하지 않은 불공정한 결과가 드러날 수 있습니다.

셋째, 인간이 결정을 내리든 기계가 결정을 내리든 공정성에 대한 표준 정의가 없습니다. 시스템에 대한 적절한 공정성 기준을 식별하려면 사용자 경험, 문화적, 사회적, 역사적, 정치적, 법적 및 윤리적 고려 사항을 고려해야 하며, 그 중 일부는 장단점이 있을 수 있습니다. 단순해 보이는 상황에서도 사람들은 무엇이 공정한지에 대해 동의하지 않을 수 있으며, 특히 글로벌 환경에서 AI 정책을 어떤 관점에서 결정해야 하는지가 불분명할 수 있습니다. 즉, "더 공정한" 시스템을 향한 지속적인 개선을 목표로 하는 것은 가능합니다.

AI의 공정성, 형평성, 포용성을 다루는 것은 활발한 연구 분야입니다. 중요하고 다양한 지식을 구현하는 [포용적인 인력을 육성](https://about.google/belonging/)하는 것부터, 연구 개발 프로세스 초기에 커뮤니티로부터 의견을 구하여 [사회적 맥락](https://blog.research.google/2023/07/using-societal-context-knowledge-to.html?_gl=1*1q8af7k*_ga*NDQ3Mjc2MzAzLjE2OTA4MDkzMTg.*_ga_KFG60X3H7K*MTY5NDM0ODQ4Mi4xMC4xLjE2OTQzNTE1OTIuMC4wLjA.)에 대한 이해를 발전시키고, 불공정한 편견의 잠재적 원인에 대한 학습 데이터 세트를 평가하는 것까지 총체적인 접근 방식이 필요합니다. 문제가 되는 편향을 제거하거나 수정하기 위한 모델 훈련, 성능 차이에 대한 모델 평가, 불공정한 결과에 대한 최종 AI 시스템의 지속적인 적대 테스트에 이르기까지 다양합니다. 

실제로 ML 모델은 역사 전반에 걸쳐 발전하고 지속되어 긍정적인 변화를 가져오는 의식적, 무의식적 인간 편견과 포용에 대한 장벽 중 일부를 식별하는 데에도 사용할 수 있습니다.

AI의 공정성은 해결된 문제가 아니라 기회이자 도전을 제시합니다. Google은 이러한 모든 영역에서 진전을 이루고 더 큰 커뮤니티를 위한 도구, 데이터 세트 및 기타 리소스를 만들고 생성 AI 시스템 개발로 인해 발생하는 새로운 과제에 맞춰 이를 조정하기 위해 최선을 다하고 있습니다. Google의 현재 생각은 다음과 같습니다.

### Recommended practices

- 공정성과 포용성을 위한 구체적인 목표를 사용하여 모델 설계
    - 귀하의 제품에 대한 사회과학자, 인문학자 및 기타 관련 전문가와 협력하여 다양한 관점을 이해하고 설명하십시오.
    - 시간이 지남에 따라 기술과 기술 개발이 다양한 사용 사례에 어떤 영향을 미칠지 고려하십시오. 누구의 견해가 표현됩니까? 어떤 유형의 데이터가 표시됩니까? 무엇이 빠져 있나요? 이 기술은 어떤 결과를 가능하게 하며 이러한 결과를 다양한 사용자 및 커뮤니티와 어떻게 비교합니까? 어떤 편견, 부정적인 경험 또는 차별적인 결과가 발생할 수 있나요?
    - 예상되는 사용 사례(예: X개의 다른 언어 또는 Y개의 다른 연령 그룹)에서 공정하게 작동하도록 시스템에 대한 목표를 설정하십시오. 시간이 지남에 따라 이러한 목표를 모니터링하고 적절하게 확장하십시오.
    - 공정성 목표를 반영하도록 알고리즘과 목적 함수를 설계하세요.
    - 기술을 사용하는 사람과 사용 방법에 따라 학습 및 테스트 데이터를 자주 업데이트하십시오.
- 모델 학습과 테스트에 대표적인 데이터세트를 사용
    - 표현 및 해당 제한 사항을 식별하고 feature, label 및 그룹 간의 편견적 또는 차별적 상관 관계를 식별하는 등 데이터세트의 공정성을 평가합니다. 시각화, 클러스터링 및 데이터 주석이 이 평가에 도움이 될 수 있습니다.
    - 시스템이 예측할 사람, 이벤트 및 속성의 실제 빈도를 더 잘 반영하기 위해 public 학습 데이터 세트를 보강해야 하는 경우가 많습니다.
    - 데이터에 주석을 추가하는 사람들의 다양한 관점, 경험 및 목표를 이해합니다. 다양한 근로자의 성공은 어떤 모습이며, 업무에 소요되는 시간과 업무 즐거움 사이의 균형은 무엇입니까?
    - 주석 팀과 협력하는 경우 긴밀하게 협력하여 지속 가능하고 다양하며 정확한 주석을 보장하는 명확한 작업, 인센티브 및 피드백 메커니즘을 설계하십시오. 예를 들어 알려진 답이 있는 표준 질문 세트를 사용하여 접근성, 근육 기억 및 주석의 편견을 포함한 인간의 가변성을 설명합니다.
- 시스템의 불공정한 편향 검사하기
    - 예를 들어, 시스템을 적대적으로 테스트할 수 있는 신뢰할 수 있고 다양한 테스터 풀을 구성하고 다양한 적대적 입력을 단위 테스트에 통합합니다. 이는 예상치 못한 부정적인 영향을 경험할 수 있는 사람을 식별하는 데 도움이 될 수 있습니다. 오류율이 낮더라도 가끔 매우 심각한 실수가 발생할 수 있습니다. 표적화된 적대적 테스트는 집계 지표에 의해 가려진 문제를 찾는 데 도움이 될 수 있습니다.
    - 시스템을 학습하고 평가하기 위한 측정항목을 설계하는 동안 다양한 하위 그룹의 성능을 검사하는 측정항목도 포함하세요. 예를 들어 하위 그룹당 위양성률과 위음성률은 어떤 그룹이 불균형적으로 더 나쁘거나 더 나은 성과를 경험하는지 이해하는 데 도움이 될 수 있습니다.
    - 분할된 통계 지표 외에도 어려운 사례에 대해 시스템의 스트레스 테스트를 수행하는 테스트 세트를 만듭니다. 이를 통해 시스템을 업데이트할 때마다 특히 해를 끼치거나 문제가 될 수 있는 예제에서 시스템이 얼마나 잘 작동하는지 빠르게 평가할 수 있습니다. 모든 테스트 세트와 마찬가지로 시스템이 발전하고 기능이 추가 또는 제거되고 사용자로부터 더 많은 피드백을 받으면 이 세트를 지속적으로 업데이트해야 합니다.
    - 이전에 시스템에서 내린 결정으로 인해 발생한 편향의 영향과 이로 인해 발생할 수 있는 피드백 루프를 고려하세요.
- 성능 분석
    - 정의한 다양한 측정항목을 고려하세요. 예를 들어, 시스템의 위양성율은 데이터의 여러 하위 그룹에 따라 다를 수 있으며, 한 측정항목의 개선이 다른 측정항목에 부정적인 영향을 미칠 수 있습니다.
    - 광범위한 사용자, 사용 사례 및 사용 컨텍스트(예: [TensorFlow 모델 분석](https://medium.com/tensorflow/introducing-tensorflow-model-analysis-scaleable-sliced-and-full-pass-metrics-5cde7baf0b7b))에 걸쳐 실제 시나리오에서 사용자 경험을 평가합니다. 먼저 dogfood에서 테스트하고 반복한 다음 출시 후에도 계속 테스트합니다.
    - 공정성 문제를 해결하기 위해 전체 시스템 설계의 모든 것이 세심하게 제작되었더라도 ML 기반 모델이 실제 라이브 데이터에 적용될 때 100% 완벽하게 작동하는 경우는 거의 없습니다. 실제 제품에서 문제가 발생하면 기존의 사회적 단점과 일치하는지 여부와 단기 및 장기 솔루션이 어떤 영향을 미칠지 고려하세요.

## Interpretability

자동화된 예측 및 의사 결정은 좋아할 만한 음악 추천부터 환자의 바이탈 사인을 지속적으로 모니터링하는 것까지 다양한 방법으로 삶을 개선할 수 있습니다. 이것이 바로 해석 가능성, 즉 AI 시스템에 질문하고, 이해하고, 신뢰할 수 있는 수준이 중요한 이유입니다. 

해석 가능성은 또한 우리의 도메인 지식과 사회적 가치를 반영하고 과학자와 엔지니어에게 모델을 설계, 개발 및 디버깅하는 더 나은 수단을 제공하며 AI 시스템이 의도한 대로 작동하는지 확인하는 데도 도움이 됩니다.

이러한 문제는 AI 시스템뿐만 아니라 인간에게도 적용됩니다. 결국 사람이 자신의 결정에 대해 만족스러운 설명을 제공하는 것이 항상 쉬운 것은 아닙니다. 예를 들어, 종양 전문의가 환자의 암이 재발했다고 생각하는 모든 이유를 정량화하는 것은 어려울 수 있습니다. 과거에 본 패턴을 바탕으로 직관을 갖고 있다고 말하면 보다 확실한 결과를 위한 후속 조치를 지시할 수도 있습니다. 

이와 대조적으로 AI 시스템은 예측에 적용된 다양한 정보를 나열할 수 있습니다. 즉, 지난 10년 동안 100명의 서로 다른 환자의 바이오마커 수준과 해당 스캔을 보일 수 있지만 모든 데이터를 결합하여 80%의 암 발병 확률을 추정하고 PET 스캔을 권장한 방법을 전달하는 데는 어려움을 겪습니다. 생성 AI 시스템의 기초가 되는 심층 신경망과 같은 복잡한 AI 모델을 이해하는 것은 기계 학습 전문가에게도 어려울 수 있습니다.

AI 시스템을 이해하고 테스트하는 것은 특히 생성 AI 모델과 시스템이 계속 등장함에 따라 기존 소프트웨어에 비해 새로운 과제를 제시합니다. 기존 소프트웨어는 본질적으로 일련의 if-then 규칙이며, 성능을 해석하고 디버깅하는 것은 주로 갈림길에서 문제를 추적하는 것으로 구성됩니다. 이는 매우 어려울 수 있지만 일반적으로 사람은 코드를 통해 이동한 경로를 추적하고 주어진 결과를 이해할 수 있습니다.

AI 시스템의 "코드 경로"에는 수백만 개의 매개변수가 포함될 수 있으며 생성 AI 시스템에서는 수십억 개의 매개변수와 수학 연산이 포함될 수 있으므로 이전 소프트웨어보다 잘못된 결정으로 이어지는 특정 버그 하나를 찾아내는 것이 훨씬 어렵습니다. 

그러나 책임 있는 AI 시스템 설계를 사용하면 이러한 수백만 또는 수십억 개의 값을 학습 데이터로 추적하거나 특정 데이터 또는 기능에 대한 주의를 모델링하여 버그를 발견할 수 있습니다. 이는 전통적인 의사 결정 소프트웨어의 주요 문제 중 하나인 "마법의 숫자"(종종 개인적인 직관이나 작은 세트의 시험 사례에 기초한, 지금은 잊혀진 프로그래머가 설명 없이 설정한 결정 규칙 또는 임계값)의 존재와 대조됩니다.

전반적으로 AI 시스템은 기본 학습 데이터, 학습 프로세스, 결과 AI 모델을 통해 가장 잘 이해됩니다. 이는 새로운 과제를 제기하지만, 선제적이고 책임 있는 가이드라인, 모범 사례 및 도구를 공식화하기 위한 기술 커뮤니티의 공동 노력은 AI 시스템을 이해하고 제어하고 디버그하는 능력을 꾸준히 향상시키고 있습니다. 우리는 이 분야에 대한 우리의 현재 작업과 생각 중 일부를 공유하고 싶습니다.

### Recommended practices

해석성과 책임성은 Google과 광범위한 AI 커뮤니티에서 진행 중인 연구 개발 영역입니다. 여기서는 현재까지 권장되는 몇 가지 사례를 공유합니다.

- 해석 가능성을 추구하기 위한 옵션을 계획하기
    
    해석 가능성을 추구하는 것은 모델을 설계하고 훈련하기 전, 도중, 후에 이루어질 수 있습니다.
    
    - 실제로 어느 정도의 해석성이 필요합니까? 모델(예: 의료, 소매 등)에 대한 관련 도메인 전문가와 긴밀히 협력하여 어떤 해석 기능이 필요한지, 그 이유를 파악하세요. 드물기는 하지만 충분한 경험적 증거가 있어 세부적인 해석이 필요하지 않은 일부 사례/시스템이 있습니다.
    - 훈련/테스트 데이터를 분석할 수 있습니까? 예를 들어 개인 데이터로 작업하는 경우 입력 데이터를 조사할 수 있는 액세스 권한이 없을 수 있습니다.
    - 훈련/테스트 데이터를 변경할 수 있습니까? 예를 들어 특정 하위 집합(예: 기능 공간의 부분/슬라이스)에 대한 추가 훈련 데이터를 수집하거나 관심 있는 범주에 대한 테스트 데이터를 수집할 수 있습니까?
    - 새로운 모델을 디자인할 수 있습니까? 아니면 이미 훈련된 모델로 제한되어 있습니까?
    - 너무 많은 투명성을 제공하여 잠재적으로 남용의 여지가 있습니까?
    - 학습 후 해석 가능성 옵션은 무엇입니까? 모델의 내부 요소(예: 블랙박스와 화이트박스)에 접근할 수 있나요?
- 해석 가능성을 사용자 경험의 핵심 부분으로 취급
    - 개발 주기에서 사용자와 함께 반복하여 사용자 요구 사항과 목표에 대한 가정을 테스트하고 개선합니다.
    - 사용자가 AI 시스템의 유용한 정신 모델을 구축할 수 있도록 UX를 디자인합니다. 명확하고 설득력 있는 정보가 제공되지 않으면 사용자는 AI 시스템 작동 방식에 대한 자신만의 이론을 구성할 수 있으며, 이는 시스템 사용 방법에 부정적인 영향을 미칠 수 있습니다.
    - 가능하다면 사용자가 자신의 민감도 분석을 쉽게 수행할 수 있도록 하십시오. 즉, 다양한 입력이 모델 출력에 어떤 영향을 미치는지 테스트할 수 있는 권한을 부여하십시오.
    - 추가 관련 UX 리소스: [인간의 요구에 맞는 디자인](https://medium.com/google-design/human-centered-machine-learning-a770d10562cd), [사용자 제어](https://design.google/library/ux-ai), [AI 교육](https://design.google/library/designing-and-learning-teachable-machine), [습관화](https://design.google/library/predictably-smart), [공정성](https://design.google/library/fair-not-default), [표현](https://blog.research.google/2017/05/neural-network-generated-illustrations.html?m=1)
- 해석 가능하도록 모델 설계
    - 성능 목표에 필요한 최소한의 입력 세트를 사용하여 어떤 요인이 모델에 영향을 미치는지 더 명확하게 만듭니다.
    - 성능 목표를 충족하는 가장 간단한 모델을 사용하십시오.
    - 가능하다면 상관관계가 아닌 인과관계를 배우십시오(예: 어린이가 롤러코스터를 탈 수 있는지 예측하려면 나이가 아닌 키를 사용하십시오).
    - 실제 목표와 일치하는 훈련 목표를 만듭니다(예: 정확도가 아닌 허위 경보의 허용 가능한 확률에 대한 훈련).
    - 도메인 전문 지식을 반영하는 입력-출력 관계를 생성하도록 모델을 제한합니다(예: 커피숍은 사용자에게 더 가깝고 다른 모든 것이 동일하다면 추천될 가능성이 더 높아야 합니다).
- 최종 목표와 최종 작업을 반영하는 측정항목을 선택하기
    - 고려하는 측정항목은 특정 상황의 특정 이점과 위험을 다루어야 합니다. 예를 들어, 화재 경보 시스템은 가끔 허위 경보가 발생하더라도 재현율이 높아야 합니다.
- 학습된 모델 이해
    
    모델에 대한 통찰력(예: 입력에 대한 민감도)을 얻기 위해 많은 기술이 개발되고 있습니다.
    
    - 다양한 예시 하위 집합에 대해 다양한 입력에 대한 모델의 민감도를 분석합니다.
- 모델 사용자에게 설명 전달
    - 사용자가 이해할 수 있고 적절한 설명을 제공합니다(예: 기술적인 세부 사항은 업계 종사자와 학계에 적합할 수 있지만 일반 사용자는 UI 프롬프트, 사용자 친화적인 요약 설명 또는 시각화가 더 유용할 수 있습니다). 설명은 다양한 맥락에서 무엇이 좋은 설명으로 간주되는지에 대한 철학적, 심리학적, 컴퓨터 과학(HCI 포함), 법적, 윤리적 고려 사항을 신중하게 고려하여 이루어져야 합니다.
    - 설명이 적절하지 않은지 여부와 지점을 식별합니다(예: 설명이 일반 사용자에게 더 많은 혼란을 초래할 수 있는 경우, 사악한 행위자가 시스템 또는 사용자 남용에 대한 설명을 이용할 수 있거나 설명이 독점 정보를 노출할 수 있는 경우).
    - 특정 사용자 기반에서 설명을 요청했지만 제공할 수 없거나 제공해서는 안 되는 경우, 또는 명확하고 건전한 설명을 제공하는 것이 불가능한 경우 대안을 고려하세요. 대신 감사와 같은 다른 메커니즘을 통해 책임을 제공하거나 사용자가 결정에 이의를 제기하도록 허용하거나 피드백을 제공하여 향후 결정이나 경험에 영향을 미칠 수 있게 합니다.
    - 앞으로 부정확한 예측을 수정하기 위해 사용자가 취할 수 있는 명확한 조치를 제안하는 설명에 우선순위를 지정하세요.
    - 설명이 인과관계를 의미한다고 암시하지 마세요.
    - 인간의 심리와 한계(예: 확증 편향, 인지 피로)를 인식합니다.
    - 설명은 다양한 형태(예: 텍스트, 그래프, 통계)로 나타날 수 있습니다. 시각화를 사용하여 통찰력을 제공하는 경우 HCI 및 시각화의 모범 사례를 사용하세요.
    - 집계된 요약에서는 정보가 손실되고 세부 정보가 숨겨질 수 있습니다(예: 부분 종속성 도표).
    - ML 시스템의 부분(특히 입력)과 모든 부분이 함께 작동하는 방식('완전성')을 이해하는 능력은 사용자가 시스템의 보다 명확한 정신적 모델을 구축하는 데 도움이 됩니다. 이러한 정신 모델은 실제 시스템 성능과 더욱 밀접하게 일치하여 보다 신뢰할 수 있는 경험과 향후 학습에 대한 보다 정확한 기대치를 제공합니다.
    - 설명의 한계에 유의하세요(예: 지역적 설명은 광범위하게 일반화되지 않을 수 있으며 시각적으로 유사한 두 가지 예에 대해 상충되는 설명을 제공할 수 있음).
- 테스트, 테스트, 테스트
    
    소프트웨어 엔지니어링 모범 사례 및 품질 엔지니어링을 통해 AI 시스템이 의도한 대로 작동하고 신뢰할 수 있는지 확인하세요.
    
    - 엄격한 단위 테스트를 수행하여 시스템의 각 구성 요소를 개별적으로 테스트합니다.
    - AI 시스템에 대한 입력 통계를 테스트하여 입력 드리프트를 사전에 감지하여 예상치 못한 방식으로 변경되지 않는지 확인합니다.
    - 완벽한 표준 데이터 세트를 사용하여 시스템을 테스트하고 예상대로 계속 작동하는지 확인하세요. 사용자 및 사용 사례 변화에 맞춰 이 테스트 세트를 정기적으로 업데이트하고 테스트 세트에 대한 교육 가능성을 줄입니다.
    - 개발 주기에 다양한 사용자 요구 사항을 통합하기 위해 반복적인 사용자 테스트를 수행합니다.
    - [포카요케](https://en.wikipedia.org/wiki/Poka-yoke)의 품질 엔지니어링 원칙을 적용합니다. 의도하지 않은 오류가 발생하지 않도록 하거나 즉각적인 대응을 촉발하지 못하게 하도록 품질 검사를 시스템에 구축합니다(예: 중요한 기능이 예기치 않게 누락된 경우 AI 시스템이 예측을 출력하지 않습니다).
    - 통합 테스트 수행: AI 시스템이 다른 시스템과 상호 작용하는 방식과 피드백 루프가 생성되는 경우가 무엇인지 이해합니다(예: 인기 있는 뉴스 기사를 추천하면 해당 뉴스 기사가 더 유명해져서 더 많이 추천될 수 있습니다).

## Privacy

ML 모델은 학습 데이터로부터 학습하고 입력 데이터를 바탕으로 예측합니다. 때로는 학습 데이터, 입력 데이터 또는 둘 다 매우 민감할 수 있습니다. 

민감한 데이터(예: 책임 있게 소싱된 생체 검사 이미지 데이터 세트로 훈련되고 개별 환자 스캔에 배포되는 암 탐지기)를 기반으로 작동하는 모델을 구축하면 엄청난 이점이 있을 수 있지만, 민감한 데이터를 사용할 때 잠재적인 개인 정보 보호 영향을 고려하는 것이 필수적입니다. 

여기에는 법적 및 규제 요구 사항을 존중하는 것뿐만 아니라 사회적 규범과 일반적인 개인 기대치를 고려하는 것도 포함됩니다. 예를 들어, ML 모델이 노출된 데이터의 측면을 기억하거나 공개할 수 있다는 점을 고려하여 개인의 개인정보를 보호하기 위한 보호 장치를 마련하는 것이 중요합니다. 사용자에게 데이터에 대한 투명성과 제어권을 제공하는 것이 중요합니다.

다행히도 다양한 기법을 정확하고 원칙에 입각한 방식으로 적절하게 적용하면 ML 모델이 그 아래에 깔려 있는 데이터를 드러낼 가능성을 최소화할 수 있습니다. Google은 생성 AI 시스템에 대한 새로운 관행을 포함하여 AI 시스템의 개인정보를 보호하기 위한 기술을 지속적으로 개발하고 있습니다. 이는 AI 커뮤니티에서 활발하게 연구되고 있는 분야이며 지속적인 성장 여지가 있습니다. 아래에서는 지금까지 배운 교훈을 공유합니다.

### Recommended practices

모든 ML 작업에 단일한 "올바른" 모델이 없는 것처럼 모든 시나리오에서 ML 개인 정보 보호에 대한 단일한 올바른 접근 방식이 없으며 새로운 접근 방식이 발생할 수 있습니다. 실제로 연구원과 개발자는 당면한 작업에 대해 개인 정보 보호와 유용성의 적절한 균형을 맞추는 접근 방식을 찾기 위해 반복해야 합니다. 이 프로세스가 성공하려면 [직관적이고 형식적으로](https://arxiv.org/abs/1802.08908) 정확할 수 있는 개인 정보 보호에 대한 명확한 정의가 필요합니다.

- 책임감 있게 데이터를 수집하고 처리합니다.
    - 민감한 데이터를 사용하지 않고(예: 민감하지 않은 데이터 수집 또는 기존 공개 데이터 소스 활용) ML 모델을 학습할 수 있는지 확인하세요.
    - 민감한 훈련 데이터를 처리하는 것이 필수적인 경우 해당 데이터의 사용을 최소화하도록 노력하십시오. 민감한 데이터를 주의 깊게 처리하세요. 예를 들어 필수 법률 및 표준을 준수하고, 사용자에게 명확한 알림을 제공하고, 데이터 사용에 대해 필요한 제어권을 제공하고, 전송 및 저장 중 암호화와 같은 [모범 사례](https://business.safety.google/compliance/)를 따르고, [Google 개인정보 보호 원칙](https://business.safety.google/compliance/)을 준수합니다.
    - 모범 사례 데이터 스크러빙 파이프라인을 사용하여 수신 데이터를 익명화하고 집계합니다. 예를 들어 개인 식별 정보(PII) 및 비익명화를 허용할 수 있는 이상값 또는 메타데이터 값(도착 순서와 같은 암시적 메타데이터 포함, 무작위 섞기로 제거 가능)을 제거하는 것을 고려합니다. [Prochlo](https://arxiv.org/abs/1710.00901) 또는 [Cloud Data Loss Prevention](https://cloud.google.com/dlp?hl=ko) API를 사용하여 민감하고 식별 가능한 데이터를 자동으로 검색하고 수정합니다.
- 적절한 경우 온디바이스 처리 활용
    - 목표가 개별 상호 작용에 대한 통계(예: 특정 UI 요소가 사용되는 빈도)를 학습하는 것이라면 민감한 정보가 포함될 수 있는 원시 상호 작용 데이터보다는 기기 내에서 로컬로 계산된 통계만 수집하는 것이 좋습니다.
    - 여러 장치가 로컬에 저장된 훈련 데이터에서 공유 글로벌 모델을 훈련하기 위해 조정하는 [연합 학습](https://blog.research.google/2017/04/federated-learning-collaborative.html)과 같은 기술이 시스템의 개인 정보 보호를 향상시킬 수 있는지 고려하십시오.
    - 가능하다면 기기에서 집계, 무작위화 및 스크러빙 작업을 적용합니다(예: 보안 집계, RAPPOR 및 Prochlo의 인코딩 단계). 이러한 작업은 사용된 기술에 증거가 수반되지 않는 한 실용적이고 최선의 개인 정보 보호만 제공할 수 있습니다.
- ML 모델의 개인정보를 적절하게 보호합니다.
    
    ML 모델은 내부 매개변수는 물론 외부에 표시되는 동작을 통해 학습 데이터에 대한 세부정보를 노출할 수 있으므로 모델 구성 및 액세스 방법이 개인정보 보호에 미치는 영향을 고려하는 것이 중요합니다.
    
    - ["노출" 측정](https://arxiv.org/abs/1802.08232) 또는 [멤버십 추론 평가](https://arxiv.org/abs/1610.05820)를 기반으로 한 테스트를 사용하여 모델이 의도치 않게 민감한 데이터를 기억하거나 노출하는지 여부를 추정합니다. 이러한 측정항목은 모델 유지 관리 중 회귀 테스트에 추가로 사용될 수 있습니다.
    - 데이터 최소화를 위한 매개변수(예: 집계, 이상값 임계값, 무작위 요인)를 실험하여 장단점을 이해하고 모델에 대한 최적의 설정을 식별합니다.
    - 개인 정보 보호에 대한 수학적 보장을 설정하는 기술을 사용하여 ML 모델을 학습합니다. 이러한 분석 보장은 전체 운영 시스템을 보장하지 않습니다.
    - 원칙적이고 입증 가능한 접근 방식의 사용, 동료 검토를 거친 새로운 아이디어 출판, 중요한 소프트웨어 구성 요소의 오픈 소스화, 모든 단계에서 검토를 위한 디자인과 개발의 전문가 모집 등 암호화 및 보안이 중요한 소프트웨어에 대해 확립된 모범 사례 프로세스를 따릅니다.

## Safe and security

안전과 보안에는 공격자가 어떻게 방해하려고 하든 AI 시스템이 의도한 대로 작동하도록 보장하는 것이 포함됩니다. AI 시스템이 안전이 중요한 애플리케이션에 널리 사용되기 전에 AI 시스템의 안전을 고려하고 해결하는 것이 필수적입니다. 

AI 시스템의 안전과 보안에는 고유한 많은 과제가 있습니다. 예를 들어, 인간이 해결하기 어려운 문제에 ML이 적용되는 경우, 특히 생성 AI 시대에는 모든 시나리오를 미리 예측하기가 어렵습니다. 또한 안전을 위해 필요한 사전 제한과 창의적인 솔루션을 생성하거나 비정상적인 입력에 적응하는 데 필요한 유연성을 모두 제공하는 시스템을 구축하는 것도 어렵습니다. AI 기술이 발전함에 따라 보안 문제도 발전할 것입니다. 공격자는 반드시 새로운 공격 수단을 찾을 것입니다. 그리고 새로운 솔루션도 함께 개발되어야 합니다. 다음은 지금까지 배운 내용을 바탕으로 한 현재 권장 사항입니다.

### Recommended Practices

ML의 안전 연구는 학습 데이터 포이즈닝, 민감한 학습 데이터 복구, 모델 도용, 적대적 보안 사례를 포함한 광범위한 위협을 포괄합니다. Google은 이러한 모든 영역과 관련된 연구에 투자하고 있으며 이 작업 중 일부는 AI 개인정보 보호 관행과 관련되어 있습니다. Google 안전 연구의 초점 중 하나는 적대적 학습이었습니다. 즉, 하나의 신경망을 사용하여 시스템을 속일 수 있는 적대적 사례를 생성하고 두 번째 네트워크와 결합하여 사기를 탐지하는 것입니다.

현재, 적대적인 사례에 대한 최선의 방어는 아직 프로덕션 환경에서 사용하기에 충분히 안정적이지 않습니다. 이는 [지속적이고 매우 활발한](https://arxiv.org/abs/1802.00420) 연구 분야입니다. 아직 효과적인 방어 수단이 없기 때문에 개발자는 시스템이 공격을 받을 가능성이 있는지, 성공적인 공격으로 인해 발생할 수 있는 결과를 고려하고 대부분의 경우 이러한 공격이 심각한 부정적인 영향을 미칠 수 있는 시스템을 구축해서는 안 됩니다.

또 다른 관행은 텍스트 생성 모델에게 특정 종교에 대한 혐오 표현을 생성하게 하는 것과 같은, 악의적이거나 부주의하게 유해한 입력이 제공될 때 ML 모델 또는 애플리케이션이 어떻게 작동하는지 학습할 목적으로 ML 모델 또는 애플리케이션을 체계적으로 평가하는 방법인 적대적 테스트입니다. 이 관행은 팀이 현재 실패 패턴을 노출하여 모델과 제품을 체계적으로 개선하고 완화 경로를 안내하는 데 도움이 됩니다(예: 모델 미세 조정, 입력 또는 출력에 대한 필터 또는 기타 보호 조치 마련). 가장 최근에 우리는 AI 시스템을 ["윤리적으로 해킹"](https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/)하고 [보안 AI 프레임워크](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)를 지원하기 위해 공격에 대한 취약성을 식별하는 적대적 보안 테스트 접근 방식인 지속적인 "레드 티밍" 노력을 발전시켰습니다.

- 시스템에 대한 잠재적 위협 식별
    - 누군가 시스템을 오작동하게 만들 동기가 있는지 고려하십시오. 예를 들어 개발자가 사용자가 자신의 사진을 정리하는 데 도움이 되는 앱을 구축하는 경우 사용자가 사진을 수정하여 잘못 정리하게끔 만드는 것이 쉬울 수 있지만 그렇게 할 동기가 제한적일 수 있습니다.
    - 시스템 실수로 인해 발생할 수 있는 의도하지 않은 결과를 식별하고 이러한 결과의 가능성과 심각도를 평가합니다.
    - 가능한 모든 공격 벡터를 이해하기 위해 엄격한 위협 모델을 구축하십시오. 예를 들어 공격자가 ML 모델에 대한 입력을 변경할 수 있도록 허용하는 시스템은 사용자가 수행한 작업의 타임스탬프와 같이 서버에서 수집한 메타데이터를 처리하는 시스템보다 훨씬 더 취약할 수 있습니다. 직접적인 참여 없이 수집된 입력 기능을 의도적으로 수정하는 것이 더 어렵기 때문입니다.
- 위협에 맞서기 위한 접근 방식 개발
    
    스팸 필터링과 같은 일부 애플리케이션은 적대적 ML의 어려움에도 불구하고 현재 방어 기술로 성공할 수 있습니다.
    
    - 적대적인 환경에서 시스템의 성능을 테스트하십시오. 어떤 경우에는 [CleverHans](https://github.com/cleverhans-lab/cleverhans)와 같은 도구를 사용하여 이 작업을 수행할 수 있습니다.
    - 내부 [레드팀](https://en.wikipedia.org/wiki/Red_team)을 만들어 테스트를 수행하거나, 제3자가 시스템을 적대적으로 테스트하도록 장려하는 콘테스트나 포상금 프로그램을 주최하세요.
- 앞서 나가기 위해 계속 학습하기
    - 최신 연구 발전에 대한 최신 정보를 받아보세요. 적대적 기계 학습에 대한 연구는 지속적으로 [향상된 방어 성능](https://arxiv.org/abs/1803.06373)을 제공하며 일부 방어 기술은 [입증 가능한 보장](https://arxiv.org/abs/1711.00851)을 제공하기 시작했습니다.
    - 입력을 방해하는 것 외에도 [ML 공급망에 다른 취약점](https://arxiv.org/abs/1708.06733)이 있을 수 있습니다. 우리가 아는 한 이러한 공격은 아직 발생하지 않았지만 가능성을 고려하고 대비하는 것이 중요합니다.